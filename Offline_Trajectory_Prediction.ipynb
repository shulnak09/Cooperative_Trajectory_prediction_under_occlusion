{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\n2kan\\OneDrive - Virginia Tech\\Anshul\\Research\\Ph.D\\Uncertainty_aware_planning\\Anshul_Linux\\ZED2_camera_ws\\Offline_Trajectory_Prediction.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/n2kan/OneDrive%20-%20Virginia%20Tech/Anshul/Research/Ph.D/Uncertainty_aware_planning/Anshul_Linux/ZED2_camera_ws/Offline_Trajectory_Prediction.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinalg\u001b[39;00m \u001b[39mimport\u001b[39;00m inv\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/n2kan/OneDrive%20-%20Virginia%20Tech/Anshul/Research/Ph.D/Uncertainty_aware_planning/Anshul_Linux/ZED2_camera_ws/Offline_Trajectory_Prediction.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpkl\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/n2kan/OneDrive%20-%20Virginia%20Tech/Anshul/Research/Ph.D/Uncertainty_aware_planning/Anshul_Linux/ZED2_camera_ws/Offline_Trajectory_Prediction.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import inv\n",
    "import pickle as pkl\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import glob\n",
    "import os\n",
    "from tqdm import trange\n",
    "import torch.optim as optim\n",
    "import matplotlib.font_manager\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.patches import Ellipse\n",
    "from scipy.spatial.transform import Rotation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cpu' if torch.cuda.is_available() else 'cuda')\n",
    "\n",
    "\n",
    "\n",
    "class lstm_encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=2):\n",
    "        super(lstm_encoder,self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size,\n",
    "                            num_layers = num_layers, batch_first = False)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, self.hidden = (self.lstm(x.view(x.shape[0], x.shape[1], self.input_size)))\n",
    "        return lstm_out, self.hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size),\n",
    "                  torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
    "        \n",
    "class lstm_decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers =2):\n",
    "        super(lstm_decoder,self).__init__()\n",
    "        \n",
    "        \n",
    "        self.input_size =  input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size,\n",
    "                            num_layers = num_layers, batch_first = False)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size,  input_size + 2)\n",
    "#         self.linear = nn.Linear(hidden_size, input_size)\n",
    "            \n",
    "    \n",
    "    def forward(self, x, encoder_hidden_states):\n",
    "        \n",
    "        # Shape of x is (N, features), we want (1,N, features)\n",
    "        lstm_out, self.hidden = (self.lstm(x.unsqueeze(0), encoder_hidden_states))\n",
    "        lstm_out = lstm_out\n",
    "        output = self.linear(lstm_out.squeeze(0))\n",
    "        \n",
    "        return output, self.hidden\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class lstm_seq2seq(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        \n",
    "        super(lstm_seq2seq, self).__init__()\n",
    "        self.input_size =  input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.encoder = lstm_encoder(input_size = input_size, hidden_size = hidden_size).to(device)\n",
    "        self.decoder = lstm_decoder(input_size = input_size, hidden_size = hidden_size).to(device)\n",
    "    \n",
    "    \n",
    "    def train_model(self, input_tensor, target_tensor, n_epochs, target_len, batch_size =64, beta = 0.5, \n",
    "                    training_prediction ='recursive', teacher_forcing_ratio = 0.5, learning_rate = 0.001, dynamic_tf = False):\n",
    "        \n",
    "        '''\n",
    "        input_tensor = input_data with shape (Batch, seq_length, input_size =4 )\n",
    "        target_tensor = target_data with shape(Batch, target_length, output_size = 4)\n",
    "        n_epochs = number of epochs\n",
    "        training_prediction = type of prediction that the NN model has to perform either 'recursive' or \n",
    "                        student-teacher-forcing\n",
    "        dynamic_tf =  dynamic tecaher forcing reduces the amount of teacher force ratio every epoch\n",
    "        '''\n",
    "        \n",
    "        min_logvar, max_logvar = -4, 4\n",
    "\n",
    "    \n",
    "        # define optimizer\n",
    "        optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
    "#         scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda = lambda epoch: 0.95)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "                        factor=0.25, patience=5, threshold=0.001, threshold_mode='abs')\n",
    "        \n",
    "        # Initialize loss for each epoch\n",
    "        losses = np.full(n_epochs, np.nan)\n",
    "#            criterion = nn.MSELoss()\n",
    "        \n",
    "        # Number of batches:\n",
    "        n_batch = int(input_tensor.shape[0]/batch_size)\n",
    "        num_fea = input_tensor.shape[2]\n",
    "        lrs = [] # Obtain the LR\n",
    "        \n",
    "        with trange(n_epochs) as tr:\n",
    "            for it in tr:\n",
    "                \n",
    "                batch_loss = 0\n",
    "                batch_loss_tf = 0\n",
    "                batch_loss_no_tf = 0\n",
    "                num_tf = 0\n",
    "                num_no_tf = 0\n",
    "                \n",
    "                for batch_x, batch_y in train_loader:\n",
    "                    batch_x, batch_y = batch_x.permute(1, 0, 2), batch_y.permute(1, 0, 2) # shape : seq, batch, input\n",
    "                    \n",
    "#                     print(batch_x.shape, batch_y.shape)\n",
    "                    # Initialize output tensor:\n",
    "                    outputs = torch.zeros(target_len, batch_y.shape[1], batch_y.shape[2] + 2).to(device)\n",
    "                    \n",
    "                    # Initialize the hidden state \n",
    "                    encoder_hidden = self.encoder.init_hidden(batch_y.shape[1])\n",
    "                    \n",
    "                    # zero the gradient:\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # Encoder outputs for the entire sequence of look-back:\n",
    "                    encoder_output, encoder_hidden = self.encoder(batch_x)\n",
    "                    \n",
    "#                     print(\"encoder_output\", encoder_output.shape)\n",
    "#                     print(\"encoder hidden\", encoder_hidden[0].shape)\n",
    "#                     print(\"encoder cell state\", encoder_hidden[1].shape)\n",
    "                    \n",
    "                    # Decoder outputs:\n",
    "                    decoder_input = batch_x[-1,:,:] \n",
    "#                     decoder_input_var = torch.ones_like(decoder_input)*min_var\n",
    "#                     decoder_input = torch.cat([decoder_input, decoder_input_var], dim=1)\n",
    "#                     print(decoder_input.shape)\n",
    "                    \n",
    "                    decoder_hidden = encoder_hidden\n",
    "#                     print(decoder_hidden[0].shape)\n",
    "                    \n",
    "                    if training_prediction == 'recursive':\n",
    "                        # Predict recursively:\n",
    "                        for t in range(target_len):\n",
    "                            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                            outputs[t] = decoder_output\n",
    "                            decoder_input = decoder_output[:,:num_fea]\n",
    "                            \n",
    "                    if training_prediction == 'tecaher_forcing':\n",
    "                        \n",
    "                        if random.random() < teacher_forcing_ratio:\n",
    "                            for t in range(target_len):    \n",
    "                                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                                outputs[t] = decoder_output\n",
    "                                decoder_input = torch.squeeze(batch_y[t,:,:])\n",
    "                        \n",
    "                        else:\n",
    "                            for t in range(target_len):\n",
    "                                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                                outputs[t] = decoder_output\n",
    "                                decoder_input = decoder_output[:,:num_fea]\n",
    "                    \n",
    "                    # compute the loss:\n",
    "#                     outputs = outputs.to(device, dtype=torch.float64)\n",
    "                    F, B, _ = outputs.shape\n",
    "                    \n",
    "                    #predictive UQ\n",
    "                    outputs_mean, outputs_var = outputs[:,:,:int(num_fea/2)], outputs[:,:,num_fea:] #target_len, b, 8\n",
    "#                     print( outputs_mean.shape, outputs_var.shape )\n",
    "                    outputs_logvar = torch.clamp(outputs_var, min=min_logvar, max=max_logvar)\n",
    "                    loss_NLL = 0.5*((outputs_mean - batch_y[:,:,:int(num_fea/2)])**2/torch.exp(outputs_logvar))+ 0.5*outputs_logvar\n",
    "                    if beta > 0:\n",
    "                        loss = loss_NLL * torch.exp(outputs_logvar).detach() ** beta\n",
    "                    loss_NLL = torch.mean(loss)\n",
    "                    \n",
    "                    #State UQ\n",
    "                    state_log_covar = (outputs[:,:,int(num_fea/2):num_fea])\n",
    "                    state_covar = torch.exp(state_log_covar)\n",
    "                    loss_MSE = torch.mean(0.5*(state_covar - batch_y[:,:,int(num_fea/2):num_fea])**2)\n",
    "                    \n",
    "                    loss = loss_NLL  + loss_MSE\n",
    "                                                \n",
    "#                     (torch.log(var) + ((y - mean).pow(2))/var).sum()\n",
    "#                     loss = gaussian_nll(batch_y,outputs)\n",
    "                    batch_loss += loss.item() # Compute the loss for entire batch \n",
    "                    \n",
    "                    # Backpropagation:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                \n",
    "                # LR scheduler:\n",
    "                scheduler.step(loss)\n",
    "#                 lrs.append(scheduler.get_last_lr())\n",
    "                \n",
    "                # Loss for epoch\n",
    "                batch_loss /= n_batch\n",
    "                losses[it]  = batch_loss\n",
    "                \n",
    "                # Dynamic teacher Forcing:\n",
    "                if dynamic_tf and teacher_forcing_ratio >0:\n",
    "                    teacher_forcing_ratio = teacher_forcing_ratio -0.02\n",
    "                \n",
    "                # progress bar\n",
    "                tr.set_postfix(loss=\"{0:.3f}\".format(batch_loss))\n",
    "            \n",
    "        return losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(model, input_tensor, target_len):\n",
    "        \n",
    "        '''\n",
    "        : param input_tensor:      input data (seq_len, input_size); PyTorch tensor \n",
    "        : param target_len:        number of target values to predict \n",
    "        : return np_outputs:       np.array containing predicted values; prediction done recursively \n",
    "        '''\n",
    "        input_tensor = input_tensor.permute(1, 0, 2) #batch_first=False\n",
    "        # encode input_tensor\n",
    "        encoder_output, encoder_hidden = model.encoder(input_tensor.to(device))\n",
    "\n",
    "        # initialize tensor for predictions\n",
    "        outputs = torch.zeros(target_len, input_tensor.shape[1], input_tensor.shape[2] +2, device=device) #target_len, B, 4\n",
    "\n",
    "        # decode input_tensor\n",
    "        decoder_input = input_tensor[-1, :, :]\n",
    "#         decoder_input_var = torch.ones_like(decoder_input)*min_var\n",
    "#         decoder_input = torch.cat([decoder_input, decoder_input_var], dim=1)\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        for t in range(target_len):\n",
    "            decoder_output, decoder_hidden = model.decoder(decoder_input, decoder_hidden)\n",
    "            outputs[t] = decoder_output\n",
    "            decoder_input = decoder_output[:,:input_tensor.shape[2]]\n",
    "\n",
    "        np_outputs = outputs #.detach() #.numpy()\n",
    "\n",
    "        np_outputs = np_outputs.permute(1, 0, 2) #batch_first=False\n",
    "        return np_outputs\n",
    "\n",
    "\n",
    "# Implement Kalman Filter for state estimation:\n",
    "# X[k+1] = A * X[K] + B * u[K] + w\n",
    "\n",
    "# Define the directories where your .pkl files are located\n",
    "directory1 = './frames_goodwin/CAM_1/'\n",
    "directory2 = './frames_goodwin/CAM_2/'\n",
    "\n",
    "# Create an empty list to store the loaded data\n",
    "data_list = []\n",
    "\n",
    "# Loop through all files in first directory:\n",
    "\n",
    "\n",
    "with open('./frames_goodwin/CAM_1/traj_occ4_transf','rb') as f:\n",
    "    data= pkl.load(f)\n",
    "\n",
    "np.set_printoptions(precision =3)\n",
    "# print(data)\n",
    "\n",
    "# data = 1.5 * np.array(data)\n",
    "# data = np.insert(arr = data, obj=[1],values = 0, axis =1)\n",
    "# # print(data)\n",
    "\n",
    "# # Do the transformation:\n",
    "# R =  np.array([0.92768715, -0.04470923,  0.37067186,\n",
    "# 0.02332297,  0.99780478,  0.06198113,\n",
    "#  -0.37262929, -0.04885393,  0.92669343]).reshape(3,3)\n",
    "\n",
    "# ### first transform the matrix to euler angles\n",
    "# r =  Rotation.from_matrix(R)\n",
    "# angles = r.as_euler(\"zyx\",degrees=True)\n",
    "\n",
    "# print(\"rpy\", angles)\n",
    "\n",
    "# for i in range(10):\n",
    "\n",
    "#     sigma = np.random.normal(0, 0.5*np.abs(angles))\n",
    "#     angles += 0 * sigma\n",
    "\n",
    "# new_r = Rotation.from_euler(\"zyx\",angles,degrees=True)\n",
    "# new_rotation_matrix = new_r.as_matrix()\n",
    "\n",
    "\n",
    "\n",
    "# t =  np.array([1.16323258, 0.0667586 ,0.04029998]).reshape(3,1)\n",
    "# R,t = np.array(new_rotation_matrix),np.array(t)\n",
    "# print(\"New_Rotation_Matrix\", R)\n",
    "\n",
    "\n",
    "# # print(\"Mat Mul:\",np.matmul(R,data[1,:3].reshape(-1,1)))\n",
    "# # print(\"Mat diff:\",  (data[1,:3]))\n",
    "\n",
    "# # If RON_12: convert 1 to coordinate frame of 2 : Inverse  transformation\n",
    "# #  If RON_21: convert 2 to coordinate frame of 1: Rigid Transform\n",
    "\n",
    "# coord_transf = np.zeros((data.shape[0],3))\n",
    "# for i in range(data.shape[0]):\n",
    "#     coord_transf[i,:] =  np.squeeze(np.matmul(R.T,(data[i,:3].reshape(-1,1)- t)))\n",
    "\n",
    "# # print(coord_transf)\n",
    "\n",
    "# data = np.concatenate((coord_transf[:,[0,2]],data[:,3:]), axis = 1)\n",
    "# # print(data)\n",
    "train_traj = np.expand_dims(data, axis =0)\n",
    "train_traj = train_traj - train_traj[:,0,:]\n",
    "# print(train_traj)\n",
    "# X_train, y_train = np.split(train_traj, [8,12], axis = 1)\n",
    "\n",
    "# print(X_train)\n",
    "def get_F(dt):  \n",
    "    F = np.array([[1., 0, dt, 0],\n",
    "                  [0,  1., 0, dt],\n",
    "                  [0,  0, 1., 0],\n",
    "                  [0,  0, 0, 1]])\n",
    "    return F\n",
    "\n",
    "def get_Q(dt, var_wx, var_wy):\n",
    "    Q = np.array([[0.25*dt**4*var_wx, 0, 0.5*dt**3*var_wx,0],\n",
    "                  [0, 0.25*dt**4*var_wy, 0, 0.5*dt**3*var_wy],\n",
    "                  [0.5*dt**3*var_wx, 0, dt**2*var_wx,0],\n",
    "                  [0, 0.5*dt**3*var_wy,0, dt**2*var_wy]])\n",
    "    return Q\n",
    "\n",
    "# Define the process covariance matrix P:\n",
    "sigma_x = 1\n",
    "sigma_y = 1\n",
    "sigma_u = 0.25\n",
    "sigma_v = 0.25\n",
    "P = np.diag([sigma_x**2,sigma_y**2, sigma_u**2, sigma_v**2]) \n",
    "\n",
    "\n",
    "def Kalman_filter(\n",
    "                    X_prev, \n",
    "                    X_measured,\n",
    "                    vx, vx_dot,\n",
    "                    vy, vy_dot,\n",
    "                    dt = 0.4,\n",
    "                    var_wx = 0.5,\n",
    "                    var_wy = 0.25,\n",
    "                    P = P\n",
    "                ):\n",
    "    \n",
    "    \n",
    "#     noise_x = np.random.normal(mu, sigma, [X_prev.shape[0]])\n",
    "#     X_prev_noise = X_train[:,:] + noise_x\n",
    "#     x = X_prev_noise[0,:].T\n",
    "\n",
    "    # Define the state transition matrix:\n",
    "    F = get_F(dt)\n",
    "\n",
    "\n",
    "    # Define Process noise:\n",
    "    Q = get_Q(dt, var_wx, var_wy)\n",
    "    \n",
    "    # Define the update parameters:\n",
    "    H = np.identity(X_prev.shape[0])\n",
    "    \n",
    "    # Define the measurement covariance:\n",
    "    R = np.diag([vx**2, vy**2, vx_dot**2, vy_dot**2])\n",
    "    \n",
    "    # measurement \n",
    "    z = X_measured\n",
    "    x = X_prev\n",
    "\n",
    "    # Predict step:\n",
    "    x = F @ x \n",
    "    P = F @ P @ F.T + Q\n",
    "\n",
    "    # Update step of Kalman filter:\n",
    "    # S = H*P*H.T + R ; R is the measurement covariance matrix\n",
    "    S = H @ P @ H.T + R\n",
    "    K = P @ H.T @ inv(S)\n",
    "    y = z - H @ x\n",
    "\n",
    "    x += K @ y\n",
    "    P = P - K @ H @ P\n",
    "    return x, P\n",
    "        \n",
    "    \n",
    "\n",
    "def sample_distribution(mean, var, N=1):\n",
    "    x_sample = np.random.multivariate_normal(mean, var, N)\n",
    "    if N==1:\n",
    "        x_sample = x_sample[0,:]\n",
    "    return x_sample\n",
    "\n",
    "\n",
    "def get_trajectory(\n",
    "                    X_seq, # 8, 4\n",
    "                    mu = 0.,\n",
    "                    sigma = 0.05,\n",
    "                    # add other inputs for Kalman filter\n",
    "                    dt = 0.4,\n",
    "                    var_wx = 0.5,\n",
    "                    var_wy = 0.25\n",
    "                  ):\n",
    "    lookback = X_seq.shape[0]\n",
    "    vx, vx_dot = 0.05*np.mean([X_seq[:,0]]), 0.05*np.mean([X_seq[:,2]])\n",
    "    vy, vy_dot = 0.05*np.mean([X_seq[:,1]]), 0.05*np.mean([X_seq[:,3]]) \n",
    "    \n",
    "    # Define the state variance, P:\n",
    "    sigma_x = 0.5\n",
    "    sigma_y = 0.5\n",
    "    sigma_u = 0.25\n",
    "    sigma_v = 0.25\n",
    "    P = np.diag([sigma_x**2, sigma_y**2, sigma_u**2, sigma_v**2]) \n",
    "    \n",
    "    trajectories, mus, covs = [], [], []\n",
    "    for i in range(X_seq.shape[0]):\n",
    "        X_measured = X_seq[i,:] \n",
    "        \n",
    "        if i==0:\n",
    "            X_prev = X_seq[0,:]\n",
    "            # Add a sample noise to the first point\n",
    "            noise_x = np.random.normal(mu, sigma, [X_prev.shape[0]])\n",
    "            X_prev = X_prev + noise_x\n",
    "#             X_prev = X_prev.T\n",
    "        \n",
    "        mu, cov = Kalman_filter(X_prev, \n",
    "                                X_measured, \n",
    "                                vx, \n",
    "                                vx_dot, \n",
    "                                vy, \n",
    "                                vy_dot,\n",
    "                                #add other inputs\n",
    "                                dt = 0.4,\n",
    "                                var_wx = 0.5,\n",
    "                                var_wy = 0.25,\n",
    "                                P = P\n",
    "                                )\n",
    "        P = cov\n",
    "        mus.append(mu)\n",
    "        covs.append(cov.diagonal())\n",
    "        x_new = sample_distribution(mu, cov, N=1)\n",
    "        trajectories.append(x_new)\n",
    "        X_prev = x_new\n",
    "    \n",
    "    mus = np.array(mus)\n",
    "    covs = np.array(covs)\n",
    "    trajectories = np.array(trajectories)\n",
    "    return trajectories, mus, covs\n",
    "\n",
    "def get_N_trajectories(\n",
    "            X_seq,\n",
    "            num_traj = 100,\n",
    "            ):\n",
    "    num_trajectories, batch_mu, batch_cov = [], [], []\n",
    "    for i in range(num_traj):\n",
    "        traj, mus, covs = get_trajectory(X_seq)\n",
    "        num_trajectories.append(traj)\n",
    "        batch_mu. append(mus)\n",
    "        batch_cov.append(covs)\n",
    "    num_trajectories = np.array(num_trajectories)\n",
    "    batch_mu = np.array(batch_mu)\n",
    "    batch_cov = np.array(batch_cov)\n",
    "    \n",
    "    return num_trajectories, batch_mu, batch_cov\n",
    "\n",
    "num_traj = 7\n",
    "\n",
    "batch_traj_test, test_mu, test_cov =[], [], []\n",
    "for id in range(train_traj.shape[0]):\n",
    "    X_seq = train_traj[id,:,:]\n",
    "    trajectories, mus, covs = get_N_trajectories(X_seq, num_traj)\n",
    "    batch_traj_test.append(trajectories)\n",
    "    test_mu.append(mus)\n",
    "    test_cov.append(covs)\n",
    "\n",
    "batch_traj, train_mu, train_cov = np.array(batch_traj_test), np.array(test_mu), np.array(test_cov)\n",
    "print(batch_traj.shape)\n",
    "# print(batch_traj.shape)\n",
    "\n",
    "num_fea = 2\n",
    "batch_traj = torch.tensor(batch_traj).to(device)\n",
    "X_train_KF, y_train_KF = torch.split(batch_traj[:,:,:,:num_fea],[8,12],dim = 2)\n",
    "batch_gaussian = np.concatenate([train_mu[:,:,:,:num_fea], train_cov[:,:,:,:num_fea]], axis = 3)\n",
    "batch_gaussian = torch.tensor(batch_gaussian).float().to(device)\n",
    "batch_gaussian_input, batch_gaussian_output = torch.split(batch_gaussian, [8,12], dim =2) \n",
    "\n",
    "PATH = './MCD_models/lstm_seq2seq_eth_zara01_zara02.pt'\n",
    "num_fea = batch_gaussian_input.shape[3]\n",
    "model = lstm_seq2seq(input_size=num_fea, hidden_size =128).to(device) \n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idx = list(range(7))\n",
    "Num_ens = 3\n",
    "forward_pred = 12\n",
    "look_back = 8\n",
    "min_logvar, max_logvar = -4, 4\n",
    "preds, sigmas = [],[]\n",
    "\n",
    "\n",
    "for i in random.sample(idx,Num_ens):\n",
    "    y_train_pred = predict(model, batch_gaussian_input[:,i,:,:], forward_pred)\n",
    "    # print(y_train_pred.shape)\n",
    "    y_train_pred_mu,   y_train_state_logvar, y_train_pred_logvar = y_train_pred[:,:,:int(num_fea/2)], (y_train_pred[:,:,int(num_fea/2):num_fea]), (y_train_pred[:,:,num_fea:])#target_len, b, 8\n",
    "    y_train_state_var = torch.exp(y_train_state_logvar)\n",
    "    y_train_pred_logvar = torch.clamp(y_train_pred_logvar, min=min_logvar, max=max_logvar)\n",
    "    y_train_pred_mean = torch.cat((y_train_pred_mu, y_train_state_var),2)\n",
    "    mse_train = ((y_train_pred_mean - batch_gaussian_output[:,i,:,:num_fea])**2).mean()\n",
    "    #  print(f\"Train MSE: {mse_train}\")\n",
    "    preds.append(y_train_pred_mean)\n",
    "    sigmas.append(y_train_pred_logvar)\n",
    "\n",
    "mu_preds, sigma_preds = torch.stack(preds), torch.stack(sigmas)  # Imp to convert a torch list to tensor\n",
    "\n",
    "\n",
    "def error_covariance_ellipse(X_test, y_test, mus, sigmas, ground_cov, id_no =100):\n",
    "    \n",
    "\n",
    "    num_fea = mus.shape[3]\n",
    "    sigmas = np.exp(sigmas)\n",
    "    mu_ens = np.mean(mus, axis=0)\n",
    "    # sigma_ens = torch.sqrt((torch.sum(torch.square(mu_preds) + torch.square(sigma_preds),axis=0))/sigma_preds.shape[0] - torch.square(mu_ens))\n",
    "#     var_ens = np.mean((sigmas + mus ** 2 ), axis = 0) - mu_ens**2\n",
    "    var_aleatoric = np.mean(sigmas[:,:,:,:2], axis = 0)\n",
    "    var_epistemic = np.mean(mus[:,:,:,:2]**2, axis = 0) - mu_ens[:,:,:2]**2\n",
    "    var_ens = var_aleatoric  + var_epistemic\n",
    "#     - mu_ens[:,:,:4]**2\n",
    "#     var_epistemic = np.var(mus, axis = 0)\n",
    "    var_state_unc = (np.mean((mus[:,:,:,2:4]), axis=0)) \n",
    "    ground_cov = ground_cov.transpose(1,0,2,3)\n",
    "#     ground_cov = ground_cov.transpose(1,0,2,3)\n",
    "#     print(ground_cov.shape)\n",
    "#     var_state_unc_1 = np.mean(ground_cov[:,:,:,4:], axis = 0)\n",
    "\n",
    "    \n",
    "    #     + np.mean(sigmas[:,:,:,4:], axis = 0)\n",
    "#     total_unc = var_ens[:,:,:2] + var_state_unc\n",
    "#     print(\"Var_ens\",var_ens)\n",
    "#     print(\"var_tot\",total_unc)\n",
    "#     var_total = var_ens + var_state_unc\n",
    "    \n",
    "    # For a specific ID:\n",
    "#     id_no = 20\n",
    "#     var_ens = var_ens[:,:,:2]\n",
    "#     var_state_unc = var_state_unc[:,:,:2]\n",
    "#     var_state_unc_1 = var_state_unc_1[:,:,:2]\n",
    "\n",
    "\n",
    "    ax.scatter(X_test[id_no,:,0],  X_test[id_no,:,1], color='r',marker='^',s =15, zorder=1)\n",
    "    ax.scatter(y_test[id_no,:,0],  y_test[id_no,:,1], color='r', marker='^', s = 15, zorder=2)\n",
    "    ax.plot(mu_ens[id_no,:,0], mu_ens[id_no,:,1], color='b', marker='d', alpha=0.85, ms = 5, label = 'NN state Estimate', zorder =3 )\n",
    "\n",
    "    for pred in range(8):\n",
    "#         ax.plot(train_traj[id,point,0], train_traj[id,point,1], lw= 4 -0.15*point, ms=12., marker='*', color=\"r\", linestyle=\"dashed\")\n",
    "\n",
    "        cov = np.cov(ground_cov[:,id_no,pred,0],ground_cov[:,id_no,pred,1] ) \n",
    "        lambda_, v_ = np.linalg.eig(cov)\n",
    "        lambda_ = np.sqrt(lambda_)\n",
    "        # print(lambda_)\n",
    "    #         print(lambda_tot)\n",
    "\n",
    "        for j in range(2,3):\n",
    "            ell3 = Ellipse(xy = (ground_cov[:,id_no,pred,0].mean(), ground_cov[:,id_no,pred,1].mean()),\n",
    "                     width = (lambda_[0] ) * j* 2 ,\n",
    "                     height = (lambda_[1] ) *j* 2,\n",
    "                        angle = np.degrees(np.arctan2(v_[1, 0], v_[0, 0])),\n",
    "                         color = 'black',  lw = 0.5) \n",
    "            ell3.set_facecolor('green')\n",
    "            ell3.set_alpha(0.25)\n",
    "            # ax.add_artist(ell3)\n",
    "    ell3.set_label(\"KF state uncertainty $(2\\sigma)$\")\n",
    "   \n",
    "    \n",
    "    state_cov = []\n",
    "    pred_cov = []\n",
    "    mu = []\n",
    "    for pred in range(forward_pred): \n",
    "        \n",
    "        mean = np.squeeze(mu_ens[id_no, pred, :])\n",
    "#         print(mean[0],mean[1])\n",
    "        \n",
    "        # Total Variance:\n",
    "#         var_ens = np.squeeze(var_aleatoric[id_no, pred,:]).reshape(2,2) + np.diag(np.squeeze(var_epistemic[id_no, pred,:]))\n",
    "        # Total Variance:\n",
    "#         cov_epistemic = np.squeeze(var_epistemic[id_no, pred,:]))\n",
    "        cov_pred = np.squeeze(np.squeeze(np.diag(var_ens[id_no,pred,:])))\n",
    "#         print('cov_total', cov_pred)\n",
    "\n",
    "        # Total Variance:\n",
    "        cov_state = np.squeeze(var_state_unc[id_no, pred,:2])\n",
    "        cov_state = np.diag(np.squeeze(cov_state))\n",
    "#         print('cov_state',cov_state)\n",
    "       \n",
    "        lambda_tot, v_tot = np.linalg.eig(cov_pred)\n",
    "        lambda_tot = np.sqrt(lambda_tot)\n",
    "#         print(lambda_tot)\n",
    "        \n",
    "        lambda_ale, v_ale = np.linalg.eig(cov_state)\n",
    "        lambda_ale = np.sqrt(lambda_ale)\n",
    "#         print(lambda_ale)\n",
    "        \n",
    "        for j in range(1,2):\n",
    "            ell1 = Ellipse(xy = (mean[0], mean[1]),\n",
    "                     width = ( 1* lambda_ale[0] + lambda_tot[0]) * j* 2 ,\n",
    "                     height = ( 1*lambda_ale[1] + lambda_tot[1]) *j* 2,\n",
    "                        angle = np.rad2deg(np.arccos((v_ale[0,0]))),\n",
    "                         color = 'none',  lw = 0.5) \n",
    "            ell1.set_facecolor('blue')\n",
    "            ell1.set_alpha(0.1/j)\n",
    "            # ax.add_artist(ell1)\n",
    "            \n",
    "            ell2 = Ellipse(xy = (mean[0], mean[1]),\n",
    "                 width = (lambda_tot[0]) * j* 2,\n",
    "                 height = (lambda_tot[1]) *j* 2,\n",
    "                    angle = np.rad2deg(np.arccos((v_ale[0,0]))),\n",
    "                 color = 'none', linestyle  ='--', lw = 0.25)\n",
    "            ell2.set_facecolor('tab:olive')\n",
    "            ell2.set_alpha(0.25/j)\n",
    "            ax.add_artist(ell2)\n",
    "            \n",
    "            \n",
    "        state_cov.append(cov_state)\n",
    "        pred_cov.append(cov_pred)\n",
    "        mu.append(mean)\n",
    "    state_cov = np.array(state_cov)\n",
    "    pred_cov = np.array(pred_cov)\n",
    "    mu = np.array(mu)\n",
    "\n",
    "    print(\"mu\", mu[:,:2])\n",
    "    print(\"cov\", pred_cov)\n",
    "#     ax.scatter(X_test[id_no,:,0],  X_test[id_no,:,1], color='g',marker='o')\n",
    "#     ax.plot(y_test[id_no,:,0],  y_test[id_no,:,1], color='r',alpha=0.5, marker='^' ,label = 'Ground Truth')\n",
    "#     ax.plot(mu_ens[id_no,:,0], mu_ens[id_no,:,1], color='b', marker='d', alpha=0.85,  label = 'Predicted')\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    plt.rcParams.update({'font.size': 22})\n",
    "    \n",
    "    ax.set_xlim([-2,4])\n",
    "    ax.set_xticks([0,2])\n",
    "    ax.set_ylim([-2,7])\n",
    "    csfont = {'fontname':'P052'}\n",
    "#     hfont = {'fontname':'Nimbus Sans'}\n",
    "    ell1.set_label(\"NN State Uncertainty $(2\\sigma)$\")\n",
    "    ell2.set_label(\"NN Prediction Uncertainty $(\\sigma$)\")\n",
    "    ax.set_ylabel('y (m)', **csfont,fontsize=16, fontweight ='normal')\n",
    "    # ax.legend(loc = 'lower right')\n",
    "    ax.set_xlabel('x (m)', **csfont,fontsize=16, fontweight ='normal')\n",
    "\n",
    "\n",
    "#     plt.title('title',**csfont)\n",
    "#     plt.xlabel('xlabel', **hfont)\n",
    "#     plt.show()\n",
    "#     elif a == ax[0]:\n",
    "#         a.set_title('idx = %d' %idx)\n",
    "\n",
    "    \n",
    "    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        label.set_fontsize(16)\n",
    "        label.set_fontweight('normal')\n",
    "\n",
    "    plt.grid(\"on\", alpha = 0.25)\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=16)\n",
    "\n",
    "\n",
    "\n",
    "id_list = [0] \n",
    "print(plt.style.available)\n",
    "plt.style.use('seaborn-v0_8-white')\n",
    "# plt.figure(figsize=(1,6))\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,4), sharex=False, sharey=True)\n",
    "\n",
    "\n",
    "for id_no in id_list:\n",
    "    error_covariance_ellipse (torch.squeeze(X_train_KF).detach().cpu().numpy() ,\n",
    "                          torch.squeeze(y_train_KF).detach().cpu().numpy(), \n",
    "                          mu_preds.detach().cpu().numpy(), \n",
    "                          sigma_preds.detach().cpu().numpy(), \n",
    "                          batch_gaussian.detach().cpu().numpy(),\n",
    "                          id_no = id_no)\n",
    "\n",
    "\n",
    "# Using Dictionary to get rid of duplicate legend:\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "# plt.legend(by_label.values(), by_label.keys(), fontsize = 12, loc = 'lower left', bbox_to_anchor=( -0.05,-0.05) )\n",
    "\n",
    "plt.savefig('./frames_goodwin/Cam1_transf_occ_pred.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "# plt.savefig('/home/anshul/Downloads/ZED2_camera_ws/Cam_1_traj_pred.pdf', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
